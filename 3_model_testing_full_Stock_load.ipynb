{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "\n",
    "import warnings\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "\n",
    "#import tensorflow as tf\n",
    "from sklearn import linear_model\n",
    "from sklearn import svm  #support vector machine\n",
    "from sklearn import neighbors # nearet neighbor\n",
    "from sklearn import tree # decision tree\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from sklearn.cluster import MeanShift\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import Birch\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, precision_score, r2_score\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "\n",
    "import math as math\n",
    "import sqlite3\n",
    "from datetime import datetime\n",
    "\n",
    "# One of the questions if the data is looked at like a photograph of time....\n",
    "# Would this help in predictability\n",
    "# The end result still shows no.  A variation not tested here is using Tensor Flow convoluted neural network\n",
    "def reformat_2D_series(df, defs): # must be in order from earliest to latest\n",
    "    # defs = {col:[col1, col2], histCount:4}\n",
    "    if 'col' not in defs or 'histCount' not in defs:\n",
    "        return df # must have all the info to process\n",
    "    df2 = df[defs['col']]\n",
    "    for i in range(1,defs['histCount']):\n",
    "        for col in defs['col']:\n",
    "            df2[col + '_' + str(i)]=df[col].shift(i)\n",
    "    npA = df2.dropna().values\n",
    "    d1 = int(np.shape(npA)[0])\n",
    "    d2 = int(np.shape(npA)[1]/defs['histCount'])\n",
    "    npA = npA.reshape(d1, defs['histCount'], d2)\n",
    "    return npA\n",
    "\n",
    "# -----------  scaling, with exception of the neural network, has little effect on models\n",
    "class noScale():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def transform(self, X):\n",
    "        return X\n",
    "    def fit(self, X):pass\n",
    "    \n",
    "class scaling():\n",
    "    def __init__(self):\n",
    "        self.currentModel = ''\n",
    "        self.currentArgs = {}\n",
    "        self.models = {}\n",
    "        self.models = {'noScale':{'model':noScale},\n",
    "                       'StandardScaler':{'model':StandardScaler}}\n",
    "    def fit_transform(self, X,xAll):\n",
    "        if self.currentModel != '':\n",
    "            scaler = self.models[self.currentModel]['model']()\n",
    "            #print('60 scaler', scaler)\n",
    "            scaleTest = StandardScaler()\n",
    "            scaleTest.fit(X)\n",
    "            scaler.fit(X)\n",
    "            xAll = scaler.transform(xAll)\n",
    "        return xAll\n",
    "    def getModelList(self):\n",
    "        scaleList = []\n",
    "        for key in self.models:\n",
    "            scaleList.append(key)\n",
    "        return scaleList\n",
    "    \n",
    "# Iterates through a model list with different args.\n",
    "# Fits the training data\n",
    "# Returns prediction on for testing data\n",
    "class model_testing:\n",
    "    def __init__(self):\n",
    "        self.currentModel = ''\n",
    "        self.currentArgs = {}\n",
    "        self.models = {}\n",
    "    def set_model(self, modelName, args):\n",
    "        self.currentModel = modelName\n",
    "        self.currentArgs = args\n",
    "    def create_model(self):\n",
    "        if self.currentModel in self.models:\n",
    "            if self.currentArgs == {}:\n",
    "                model = self.models[self.currentModel]['model']()\n",
    "            else:\n",
    "                model = self.models[self.currentModel]['model'](**self.currentArgs)\n",
    "            return model\n",
    "        print(\"something went wrong\")\n",
    "        return None     \n",
    "    def train_predict(self, X_train, y_train, X_test):\n",
    "        model = None\n",
    "        if self.currentModel in self.models:\n",
    "            if self.currentArgs == {}:\n",
    "                model = self.models[self.currentModel]['model']()\n",
    "                #.fit_predict(X_train, y_train, X_test)\n",
    "                #return y_pred\n",
    "            else: # my_function(**data)\n",
    "                model = self.models[self.currentModel]['model'](**self.currentArgs)\n",
    "        if model != None:\n",
    "            model.fit(X_train, y_train)\n",
    "            pred_y = model.predict(X_test)\n",
    "            return pred_y  \n",
    "        return None\n",
    "    def getModelList(self):\n",
    "        modelKeys = []\n",
    "        for key in self.models:\n",
    "            if 'args' not in self.models[key]: \n",
    "                modelKeys.append([key])\n",
    "            else:\n",
    "                args = []\n",
    "                products = []\n",
    "                for a in self.models[key]['args']:\n",
    "                    products.append(self.models[key]['args'][a])\n",
    "                    args.append(a)\n",
    "                fullList=list(itertools.product(*products))\n",
    "                for row in fullList:#valueAlpha, valueCV....\n",
    "                    actualArgs = {}\n",
    "                    for pnt in range(len(args)):\n",
    "                        actualArgs[args[pnt]] = row[pnt]\n",
    "                    modelKeys.append([key, actualArgs])\n",
    "        return modelKeys\n",
    "    \n",
    "# Create Linear Regression models to be used in testing\n",
    "class linear_regression_models(model_testing):\n",
    "    def __init__(self):\n",
    "        model_testing.__init__(self)\n",
    "        self.models = {'LinearRegression':{'model':linear_model.LinearRegression},\n",
    "                      'Ridge':{'model':linear_model.Ridge,\n",
    "                               'args':{'alpha':[.0005,.005,.05,.1,.5]}},\n",
    "                      'RidgeCV':{'model':linear_model.RidgeCV,\n",
    "                                 'args':{'alphas':[(0.1, 1.0, 10.0),(0.01, 0.1, 1.0),(1.0, 10.0, 20.0)],\n",
    "                                        'cv':[2,3,4]}},\n",
    "                       'Lasso':{'model':linear_model.Lasso,\n",
    "                               'args':{'alpha':[.05,.5,1,1.5,2]}},\n",
    "                      'LassoLars':{'model':linear_model.LassoLars,\n",
    "                                'args':{'alpha':[.05,.5,1,1.5,2]}}\n",
    "                      }\n",
    "        \n",
    "# Create other regression models to be using in testing        \n",
    "class regression_models(model_testing):\n",
    "    def __init__(self):\n",
    "        model_testing.__init__(self)\n",
    "        self.models = {'SVR':{'model':svm.SVR},\n",
    "                      'KNeighborsRegressor':{'model':neighbors.KNeighborsRegressor},\n",
    "                           # n_neighbors : int, optional (default = 5)\n",
    "                           # weights uniform, distance\n",
    "                           # algorithm : {‘auto’, ‘ball_tree’, ‘kd_tree’, ‘brute’}, optional\n",
    "                           # leaf_size : int, optional (default = 30)\n",
    "                      'DecisionTreeRegressor':{'model':tree.DecisionTreeRegressor},\n",
    "                           # criterion=’mse’, L2 , friedman_mse, mae\n",
    "                      'GradientBoostingRegressor':{'model':GradientBoostingRegressor},\n",
    "                           # loss : {‘ls’, ‘lad’, ‘huber’, ‘quantile’}, optional (default=’ls’)\n",
    "                           # learning_rate : float, optional (default=0.1)\n",
    "                           # n_estimators : int (default=100)\n",
    "                           # criterion (default=”friedman_mse”) mse, mae\n",
    "                           # max_depth : integer, optional (default=3)\n",
    "                           # alpha : float (default=0.9)\n",
    "                      'GaussianProcessRegressor':{'model':GaussianProcessRegressor},\n",
    "                           # class sklearn.gaussian_process.GaussianProcessRegressor\n",
    "                           # (kernel=None, alpha=1e-10, optimizer=’fmin_l_bfgs_b’, \n",
    "                           # n_restarts_optimizer=0, normalize_y=False, copy_X_train=True, random_state=None)\n",
    "                      'MLPRegressor':{'model':MLPRegressor,\n",
    "                                      # MLPRegressor(hidden_layer_sizes=(100, ), \n",
    "                                      # activation= {‘identity’, ‘logistic’, ‘tanh’, ‘relu’}, default ‘relu’\n",
    "                                      # solver= {‘lbfgs’, ‘sgd’, ‘adam’}, default ‘adam’\n",
    "                                      # alpha=0.0001, batch_size=’auto’, learning_rate=’constant’, learning_rate_init=0.001, \n",
    "                                      # power_t=0.5, max_iter=200, shuffle=True, random_state=None, \n",
    "                                      # tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True, \n",
    "                                      # early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08\n",
    "                                      #n_iter_no_change=10)\n",
    "                                     'args':{\n",
    "                                         'hidden_layer_sizes':[(10,),(100,),(10,10),(100,100)],\n",
    "                                         'max_iter':[200,500,1000]\n",
    "                                     }}}\n",
    "\n",
    "# Create different clustering models to be used in testing the prediction.\n",
    "class clustering_models(model_testing):\n",
    "    def __init__(self):\n",
    "        model_testing.__init__(self)\n",
    "        self.models = {'K-Means':{'model':KMeans,\n",
    "                                 'args':{'n_clusters':[3,10,50,100,200],\n",
    "                                        'max_iter':[300, 1000]}},\n",
    "                      # (n_clusters=8, init=’k-means++’, n_init=10, max_iter=300, \n",
    "                       # tol=0.0001, precompute_distances=’auto’, verbose=0, \n",
    "                       # random_state=None, copy_x=True, n_jobs=None, algorithm=’auto’)\n",
    "                      'AffinityPropagation':{'model':AffinityPropagation},\n",
    "                               # damping=0.5, max_iter=200, \n",
    "                               # convergence_iter=15, copy=True, preference=None, \n",
    "                               # affinity=’euclidean’, verbose=False\n",
    "                      'MeanShift':{'model':MeanShift},\n",
    "                                # bandwidth=None, seeds=None, \n",
    "                                # bin_seeding=False, min_bin_freq=1, cluster_all=True, n_jobs=None\n",
    "                      'SpectralClustering':{'model':SpectralClustering,\n",
    "                                 'args':{'n_clusters':[3,10,50,100,200]}},\n",
    "                               # (n_clusters=8, eigen_solver=None, \n",
    "                               # random_state=None, n_init=10, gamma=1.0, \n",
    "                               # affinity=’rbf’, n_neighbors=10, eigen_tol=0.0, \n",
    "                               # assign_labels=’kmeans’, degree=3, coef0=1, kernel_params=None, \n",
    "                               # n_jobs=None)\n",
    "                       # Hierarchical clustering  returns a tree instead of cluster's array\n",
    "                      'DBSCAN':{'model':DBSCAN},\n",
    "                               # (eps=0.5, min_samples=5, metric=’euclidean’, \n",
    "                               # metric_params=None, algorithm=’auto’, \n",
    "                               # leaf_size=30, p=None, n_jobs=None)[source]\n",
    "                       'Birch':{'model':Birch,\n",
    "                                 'args':{'n_clusters':[3,10,50,100,200]}}\n",
    "                               # (threshold=0.5, branching_factor=50, \n",
    "                               # n_clusters=3, compute_labels=True, copy=True)\n",
    "                      }                      \n",
    "\n",
    "# Classification models on this testing where not really helpful, so left out\n",
    "class classification_models(model_testing):\n",
    "    def __init__(self):\n",
    "        model_testing.__init__(self)\n",
    "        self.models = {'SVR':{'model':svm.SVR}}   \n",
    "\n",
    "# Loading data from stock file\n",
    "def loadData(fPath, file):  \n",
    "    # ============================== load data ==============    \n",
    "    dataDF = pd.read_csv(fPath + file, index_col=0)\n",
    "    # print(dataDF.head())\n",
    "    dataDF['low']=dataDF['low']/dataDF['open']\n",
    "    dataDF['high']=dataDF['high']/dataDF['open']\n",
    "    #print(dataDF.head())\n",
    "\n",
    "    colNames = dataDF.columns\n",
    "    colPredNames = ['low', 'high']\n",
    "    colPred = []\n",
    "    for n in colPredNames:\n",
    "        colPred.append(dataDF.columns.get_loc(n))\n",
    "    # dataNP = dataDF.values\n",
    "    # print(\"226  load data dataDF.describe()\", dataDF.describe())\n",
    "    # print(\"227 is null\", dataDF.isnull().sum().sum())\n",
    "    X = dataDF.values[:-1]\n",
    "    X_last = [dataDF.values[-1]]\n",
    "    y = dataDF[colPredNames].values[1:]\n",
    "    # print('233 load data', np.shape(X), np.shape(X_last), np.shape(y))\n",
    "    # ------------------------------ end load data -------\n",
    "    return X, X_last, colNames, colPredNames, colPred\n",
    "\n",
    "def test_DataModel(X, y, colPred, modelClass, pntStart, pntMid,pntShow):\n",
    "    model = modelClass.create_model()\n",
    "    \n",
    "    model.fit(X[:pntMid],y[:pntMid])\n",
    "    pred = model.predict(X[pntMid:])\n",
    "    #pred_fut = model.predict([X[-1]])\n",
    "    pred = np.hstack( (y[pntMid:],pred, y[pntMid:]-pred)  )\n",
    "    return pred\n",
    "\n",
    "def test_DataSeries(X, y, colPred, modelClass, pntStart, pntMid,pntShow):\n",
    "    model = modelClass.create_model()\n",
    "    model.fit(X[:pntStart], y[:pntStart]) # xtrain, ytrain\n",
    "    y_pred = model.predict(X[pntStart:pntMid])\n",
    "    y_train = y[:pntMid]\n",
    "    #errors = y[pntStart+1:pntMid+1] - y_pred\n",
    "    X_train = X[:pntMid]\n",
    "    predicts = []\n",
    "    for pnt in range(pntMid,len(X)): # 1st run on next line error to create the rest of \n",
    "        X_train = np.vstack( (X_train, [X[pnt]]) )\n",
    "        y_train = np.vstack(  (y_train, y[pnt])  )\n",
    "        model = modelClass.create_model()\n",
    "        model.fit(X_train[:-1], y_train[:-1])\n",
    "        y_pred = model.predict([ X_train[-1] ])\n",
    "        error = y_train[-1] - y_pred[0]\n",
    "        predicts.append( np.hstack((y_train[-1], y_pred[0], error)) )\n",
    "    pred = np.array(predicts) \n",
    "    return pred\n",
    "\n",
    "def test_DataSeries_error(X, y, colPred, modelClass, pntStart, pntMid,pntShow):\n",
    "    model = modelClass.create_model()\n",
    "    model.fit(X[:pntStart], y[:pntStart]) # xtrain, ytrain\n",
    "    y_pred = model.predict(X[pntStart:pntMid])\n",
    "    y_train = y[pntStart:pntMid-1]\n",
    "    errors = y[pntStart:pntMid] - y_pred\n",
    "    X_train = np.hstack(( X[pntStart:pntMid-1], errors[:-1] ))\n",
    "    error = y[pntMid] - y_pred[-1]\n",
    "    predicts = []\n",
    "    for pnt in range(pntMid,len(X)): # 1st run on next line error to create the rest of \n",
    "        newRow =np.hstack( ([X[pnt]], [error])  )\n",
    "        X_train = np.vstack( (X_train, newRow) )\n",
    "        y_train = np.vstack(  (y_train, [y[pnt]])  )\n",
    "        model = modelClass.create_model()\n",
    "        model.fit(X_train[:-1], y_train[:-1])\n",
    "        y_pred = model.predict([ X_train[-1] ])\n",
    "        error = y_train[-1] - y_pred[0]\n",
    "        predicts.append( np.hstack((y_train[-1], y_pred[0], error)) )\n",
    "    pred = np.array(predicts) \n",
    "    return pred\n",
    "\n",
    "mScale = scaling() # fit_transform(self, X,xAll):\n",
    "mScaleList = mScale.getModelList()\n",
    "modelsRegLin = linear_regression_models() # train_predict(self, X_train, y_train, X_test)\n",
    "modelsRegLinList = modelsRegLin.getModelList()\n",
    "# print(modelsRegLinList)\n",
    "modelsReg = regression_models()\n",
    "modelsRegList = modelsReg.getModelList()\n",
    "\n",
    "testTypes = {'mBase':test_DataModel, 'mSeries':test_DataSeries, 'mSError':test_DataSeries_error}\n",
    "\n",
    "fPathDB = 'D:/data/stocks/model_testing6.db'\n",
    "fPath = 'D:/data/stocks/ixtradingChart_5y/'\n",
    "fileList = os.listdir(fPath)\n",
    "Q = '''CREATE TABLE IF NOT EXISTS models ( \n",
    " symbol text NOT NULL\n",
    " , active integer default 1 \n",
    " , model text\n",
    " , future integer\n",
    " , error text\n",
    " , std10 integer\n",
    " , std20 integer\n",
    " , std30 integer\n",
    " , std40 integer\n",
    " , std50 integer\n",
    " , std60 integer\n",
    " , std70 integer\n",
    " , std80 integer\n",
    " , std90 integer\n",
    " , std100 integer\n",
    " , PRIMARY KEY (symbol, model)\n",
    " )'''\n",
    "conn = sqlite3.connect(fPathDB)\n",
    "conn.execute(Q)\n",
    "conn.close()\n",
    "for file in fileList[:16]:\n",
    "    start = datetime.now()\n",
    "    stockSymb = file.split('.')[0]\n",
    "    # print(stockSymb, fPath + file)\n",
    "    # print(mScaleList)\n",
    "    for scale in mScaleList: \n",
    "        mScale.currentModel = scale\n",
    "        # print(scale)\n",
    "        # mScale.currentArgs = {}\n",
    "        X, X_last, colNames, colPredNames, colPred = loadData(fPath, file)\n",
    "        X = X[~np.isnan(X).any(axis=1)]\n",
    "        dataLength = len(X)\n",
    "        pntStart = dataLength - 300\n",
    "        pntMid = dataLength - 250\n",
    "        pntEnd = -1\n",
    "        pntShow = int(.3 * pntMid)\n",
    "        mScale.fit_transform(X[:pntMid],X)\n",
    "        for modelName in modelsRegLinList:\n",
    "            for yIncr in range(1,20): # predict next nextNext...\n",
    "                y = X[:-yIncr,colPred]\n",
    "                X_train = X[yIncr:]\n",
    "                #modelClass = modelsRegLin\n",
    "                mArgs = {}\n",
    "                if len(modelName)==2: mArgs=modelName[1]\n",
    "                modelsRegLin.set_model(modelName[0], mArgs)\n",
    "                for key in testTypes:\n",
    "                    fullName = scale+'_'+key+'_'+str(modelName)\n",
    "                    fullName=fullName.replace(',','_').replace(\".0\",'').replace(':','-')\n",
    "                    stripS = \"'[]{} . ()\"\n",
    "                    for c in stripS:\n",
    "                        fullName=fullName.replace(c,'')                    \n",
    "                    try:\n",
    "                        warnings.simplefilter('ignore')\n",
    "                        pred = testTypes[key](X_train, y, colPred, modelsRegLin, pntStart, pntMid,pntShow)\n",
    "                        std=[]\n",
    "                        for i in range(10,110,10):\n",
    "                            std.append(np.std(pred[:-i,2*len(colPred):3*len(colPred)]))   \n",
    "                        # print(len(pred),len(X_train[-len(pred):,0]), s)                        \n",
    "                        # Q = r'''select * from tracker;'''\n",
    "                        # tracker = pd.read_sql_query(Q, conn)\n",
    "                        # stockSymb, fullName, future= yIncr, std\n",
    "                        Q = \"INSERT OR REPLACE INTO models \"\n",
    "                        # Q += \"(symbol, active, model, future, error, std10\"\n",
    "                        # Q += \", std20, std30 , std40, std50, std60\"\n",
    "                        # Q += \", std70, std80, std90, std100)\"\n",
    "                        Q += \"VALUES('\"\n",
    "                        Q += stockSymb + \"', 1, '\" + fullName + \"', \"\n",
    "                        Q += str(yIncr) + \", '', \"\n",
    "                        Q += str(std[0]) + \", \" # std10 integer\n",
    "                        Q += str(std[1]) + \", \" # std20 integer\n",
    "                        Q += str(std[2]) + \", \" # std30 integer\n",
    "                        Q += str(std[3]) + \", \" # std40 integer\n",
    "                        Q += str(std[4]) + \", \" # std50 integer\n",
    "                        Q += str(std[5]) + \", \" # std60 integer\n",
    "                        Q += str(std[6]) + \", \" # std70 integer\n",
    "                        Q += str(std[7]) + \", \" # std80 integer\n",
    "                        Q += str(std[8]) + \", \" # std90 integer\n",
    "                        Q += str(std[9]) + \")\" # std100 integer\n",
    "\n",
    "                        conn = sqlite3.connect(fPathDB)\n",
    "                        cursor = conn.cursor()\n",
    "                        cursor.execute(Q)\n",
    "                        conn.commit()\n",
    "                        conn.close()\n",
    "                    except Exception as e:\n",
    "                        Q = \"INSERT OR REPLACE INTO models (symbol, active, model, future, error) VALUES('\"\n",
    "                        Q += stockSymb + \"', 0, '\" + fullName + \"', \"\n",
    "                        Q += str(yIncr) + \", '\"+str(e)+ \"')\" # std100 integer\n",
    "                        conn = sqlite3.connect(fPathDB)\n",
    "                        cursor = conn.cursor()\n",
    "                        cursor.execute(Q)\n",
    "                        conn.commit()\n",
    "                        conn.close()\n",
    "    dTm = datetime.now() - start\n",
    "    print(file, round(dTm.seconds/60), dTm.seconds, dTm.microseconds, end = '   ')\n",
    "                    \n",
    "print('---done---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fPathDB)\n",
    "conn = sqlite3.connect(fPathDB)\n",
    "cursor = conn.cursor()\n",
    "# Q = \"INSERT OR REPLACE INTO models VALUES ('b', 1, 'b', 3, 'c', 4, 'd', 5, 6, 7, 8, 9, 12, 11, 11)\"\n",
    "# cursor.execute(Q)\n",
    "# conn.commit()\n",
    "# conn.close()\n",
    "cursor = conn.cursor()\n",
    "Q = r'''SELECT * FROM models'''\n",
    "# print(cursor.execute(r'''SELECT * FROM models''').fetchall() )                      \n",
    "tracker = pd.read_sql_query(Q, conn)\n",
    "print('-----')\n",
    "print(tracker.head())\n",
    "print('-----')\n",
    "Q = \"\"\"SELECT * FROM models ORDER BY std10 DESC limit 100\"\"\"\n",
    "Q = \"\"\"SELECT *, COUNT(symbol) FROM models \n",
    "    GROUP BY symbol HAVING COUNT(symbol)>1 \n",
    "    ORDER BY std10 ASC limit 100\"\"\"\n",
    "#SELECT column, COUNT(column) FROM table GROUP BY column HAVING COUNT(column) > 1\n",
    "tracker = pd.read_sql_query(Q, conn)\n",
    "print(tracker)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fPathDB)\n",
    "conn = sqlite3.connect(fPathDB)\n",
    "cursor = conn.cursor()\n",
    "Q = \"\"\"SELECT model,AVG(std50) AS Average FROM models \n",
    "    GROUP BY model HAVING COUNT(model)>1 \n",
    "    ORDER BY Average ASC\"\"\"\n",
    "                      \n",
    "tracker = pd.read_sql_query(Q, conn)\n",
    "print(tracker.head(90))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
